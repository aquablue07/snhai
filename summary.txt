Loan Adjudication LLM Challenge: Approach and Design

1. Challenge Summary
The objective of this challenge was to design, implement, and evaluate a process for building a Large Language Model (LLM) to adjudicate loan decisions and provide explanations based on a given set of business rules in a JSON format.


2. Assumptions
The provided rules are deterministic. Assumption is made that the input will not follow similar pattern all the time.
The provided fine_tune_llm_credit_rules.json file is the complete and definitive "ground truth" for all decision-making.
The rules follow this heirarchy ( reject(critical) > reject( major) > flag)

The goal is to build a system that accurately mimics these rules while being flexible enough to accommodate more complex, subjective rules in the future.

For regulatory and auditing purposes, the provided explanations must be verifiably tied back to the specific business rules.

3. Design Decisions
Based on the requirements and iterative testing, I made the following key design decisions:

Hybrid Model Architecture: I chose a hybrid system that uses a fine-tuned classifier (FinBERT) for the decision and a deterministic script for the reasoning. This approach uses the LLM for its strength in pattern recognition while using the script to guarantee that the explanations are 100% accurate and auditable according to the business logic. This provides the flexibility of an AI model with the reliability of a rule-based system.

Model Selection (ProsusAI/finbert): I selected a BERT-based model pre-trained on a large corpus of financial text. As the task involves specific financial terminology, a domain-specific model like FinBERT has a significant "head start" and can learn the classification task more efficiently from a smaller dataset than a general-purpose model. Also,

Parameter-Efficient Fine-Tuning (LoRA): I chose to implement LoRA for the training process. LoRA is vastly more efficient in terms of computational resources (VRAM, time) and storage, as it only trains a small fraction of the model's parameters (less than 1%). It provides performance comparable to a full fine-tune, making the solution more practical and scalable.

Synthetic Data Generation: I developed a script to generate a balanced dataset from the rules. This process includes creating specific "hard cases" to teach the model about rule precedence and using descriptive reasons for APPROVE outcomes to reduce model bias. This ensures the model is trained on controlled, high-quality data that covers all decision types.

4. Results and Analysis
The fine-tuned model was evaluated on the validation set, yielding strong quantitative results and clear qualitative insights.

Quantitative Performance: The model achieved a high overall accuracy of 90% on the validation set. Performance was robust and well-balanced across all categories, with a weighted F1-score of 0.90. The model was particularly effective at identifying REJECT cases, achieving a precision of 1.00, meaning it never incorrectly rejected a valid application during the test.

Qualitative Analysis: A review of the sample predictions reveals the model's primary strengths and weaknesses:

Strengths: The model is highly effective at correctly identifying the specific feature that violates a business rule. In the vast majority of test cases, its predicted label and the accompanying reason (derived from the feature importance analysis) perfectly matched the ground truth.

Weaknesses: The model's main weakness appears in a small number of cases where it correctly identifies the feature violating a critical rule but incorrectly assigns a FLAG_REVIEW label instead of the correct REJECT label. This indicates a slight difficulty in consistently applying the rule severity hierarchy.


Notes:

Also tried google/flan-t5-small, google/flan-t5-base, AdaptLLM/finance-LLM, flang etc but did not work for various reasons.

Initially explored end-to-end generative models like FLAN-T5 to produce both a decision and a reason in a single step. However, ensuring a probabilistic model would perfectly adhere to a strict rule hierarchy proved challenging.

This led to the final hybrid approach, which was determined to be the most robust solution(considering the resources in hand- using cpu) This system uses:

A specialized classifier, ProsusAI/finbert, fine-tuned with LoRA to make the APPROVE, REJECT, or FLAG_REVIEW prediction.

To connect the model's decision to the reasoning, I implemented a simple feature-importance analysis (occlusion) after finding that more complex XAI libraries like SHAP had critical incompatibilities with the development environment.

This hybrid design was ultimately chosen because it leverages the LLM's strength in pattern recognition while using a simple script to guarantee the explanations are 100% accurate and compliant with the business logic. This provides the optimal balance of AI flexibility and the auditable reliability required for a financial application.
